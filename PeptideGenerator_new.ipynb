{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**GAN**"
      ],
      "metadata": {
        "id": "PZNppTZBwYnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1F33NulCRrq",
        "outputId": "b462618f-d109-4516-a4a5-f2481786e29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AvonYangXX1/AMPLify-Feedback.git\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "BT9cm0MDt_N8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2355f0db-8b59-43ff-a52d-62a5c5a0165d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AMPLify-Feedback'...\n",
            "remote: Enumerating objects: 453, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 453 (delta 54), reused 84 (delta 42), pack-reused 351\u001b[K\n",
            "Receiving objects: 100% (453/453), 196.31 MiB | 20.92 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "Updating files: 100% (89/89), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "def build_generator(seq_length, depth, latent_dim):\n",
        "    inputs0 = layers.Input(shape=(latent_dim,), name=\"Input0\")\n",
        "    x = layers.Dense(256, activation='relu', name=\"Dense0\")(inputs0)\n",
        "    x = layers.BatchNormalization(name=\"Norm0\")(x)\n",
        "    x = layers.Dense(256, activation='relu', name=\"Dense1\")(x)\n",
        "    x = layers.BatchNormalization(name=\"Norm1\")(x)\n",
        "    x = layers.Dense(256, activation='relu', name=\"Dense2\")(x)\n",
        "    x = layers.BatchNormalization(name=\"Norm2\")(x)\n",
        "    x = layers.Dense(256, activation='relu', name=\"Dense3\")(x)\n",
        "    x = layers.BatchNormalization(name=\"Norm3\")(x)\n",
        "    x = layers.Dense(256, activation='relu', name=\"Dense4\")(x)\n",
        "    x = layers.Dense(seq_length*depth, activation='linear', name=\"DenseResize\")(x)\n",
        "    x = layers.Reshape((seq_length, depth), name=\"Reshape\")(x)\n",
        "    # x = layers.RepeatVector(seq_length, name=\"RepeatVector\")(x)\n",
        "    # x = layers.LSTM(256, return_sequences=True, name=\"GRU0\")(x)\n",
        "    x = layers.Dense(depth, activation=\"softmax\", name=\"Output\")(x)\n",
        "    model = tf.keras.models.Model(inputs=inputs0, outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ghzvubkvwaco"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator\n",
        "def build_discriminator(seq_length, depth):\n",
        "    model = tf.keras.Sequential(name=\"discriminator\")\n",
        "    model.add(layers.Conv1D(32, 5, name=\"Conv1D\"))\n",
        "    model.add(layers.Flatten(name=\"Flatten\"))\n",
        "    model.add(layers.Dense(512, activation='relu', name=\"Dense0\"))\n",
        "    model.add(layers.Dropout(0.3, name=\"Dropout\"))\n",
        "    model.add(layers.Dense(256, activation='relu', name=\"Dense1\"))\n",
        "    model.add(layers.Dense(1, activation='sigmoid', name=\"Output\"))\n",
        "    return model"
      ],
      "metadata": {
        "id": "QMkiH1dWwe16"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GAN\n",
        "def compile_gan(generator, discriminator):\n",
        "    discriminator.compile(loss='binary_crossentropy',\n",
        "                          optimizer='adam',\n",
        "                          metrics=[tf.keras.metrics.FalsePositives(),\n",
        "                                   tf.keras.metrics.FalseNegatives()])\n",
        "    discriminator.trainable = False\n",
        "    gan_input0 = layers.Input(shape=(latent_dim,))\n",
        "    gan_output = discriminator(generator(gan_input0))\n",
        "    gan = tf.keras.Model(gan_input0, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-4))\n",
        "    return gan"
      ],
      "metadata": {
        "id": "uPTCrWg-wpBp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa_vocal = np.load(\"AMPLify-Feedback/model_weights/SeqTV_vocal.npy\")\n",
        "pep_decoder = tf.keras.layers.StringLookup(vocabulary=aa_vocal[1:], invert=True, oov_token='')"
      ],
      "metadata": {
        "id": "bZwIUT4nThDJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, gan, path, epochs, batch_size, latent_dim, demo_noise):\n",
        "    for epoch in range(epochs):\n",
        "        files = os.listdir(path)\n",
        "        for file in files:\n",
        "            seq = np.load(f\"{path}/{file}\", allow_pickle=True)\n",
        "            seq = tf.one_hot(seq.squeeze(), depth=43)\n",
        "            total_d_loss = 0\n",
        "            total_g_loss = 0\n",
        "            num_batches = int(seq.shape[0] / batch_size)\n",
        "            for i in range(0, seq.shape[0], batch_size):\n",
        "                real_sequences = seq[i:i + batch_size]\n",
        "                current_batch_size = real_sequences.shape[0]\n",
        "\n",
        "                # Generate Fake sequence\n",
        "                noise = (np.random.rand(current_batch_size, latent_dim)-0.5)*2\n",
        "                generated_sequences = generator.predict(noise, verbose=0)\n",
        "\n",
        "                # Labels for real and fake data\n",
        "                real_labels = np.ones((current_batch_size, 1))\n",
        "                fake_labels = np.zeros((current_batch_size, 1))\n",
        "\n",
        "                # Train discriminator\n",
        "                discriminator.trainable = True\n",
        "                d_loss_real = discriminator.train_on_batch(real_sequences, real_labels)\n",
        "                d_loss_fake = discriminator.train_on_batch(generated_sequences, fake_labels)\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "                discriminator.trainable = False\n",
        "\n",
        "                # Train generator\n",
        "                g_loss = gan.train_on_batch(noise, np.ones((current_batch_size, 1)))\n",
        "\n",
        "                total_d_loss += d_loss\n",
        "                total_g_loss += g_loss\n",
        "                # print(f\"Epoch {epoch+1}/{epochs}; {file}; Batch {i}/{num_batches}; FP {d_loss[1]/current_batch_size:.4f}; FN {d_loss[2]/current_batch_size:.4f}; G_loss {g_loss:.4f}\")\n",
        "\n",
        "            demo_seq = generator(demo_noise)\n",
        "            demo_seq = tf.math.argmax(demo_seq, axis=2)\n",
        "            demo_seq = pep_decoder(demo_seq).numpy().astype('str')\n",
        "            demo_seq = [\"\".join(chars) for chars in demo_seq]\n",
        "            print(demo_seq[0])\n",
        "            print(f\"Epoch {epoch+1}/{epochs}; FP {total_d_loss[1]/seq.shape[0]:.4f}; FN {total_d_loss[2]/seq.shape[0]:.4f}; G_Loss {total_g_loss/num_batches:.4f}\")\n",
        "            del seq\n",
        "            # generator.save(f\"drive/MyDrive/MIT687/Generator.keras\")\n",
        "            # discriminator.save(f\"drive/MyDrive/MIT687/Discriminator.keras\")"
      ],
      "metadata": {
        "id": "cpbOvr8Qw9g3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2\n",
        "seq_length = 190\n",
        "depth = 43\n",
        "path = \"AMPLify-Feedback/processed_data/gan_train_data\"\n",
        "np.random.seed(8701)\n",
        "demo_noise = noise = (np.random.rand(1, latent_dim)-0.5)*2\n",
        "\n",
        "generator = build_generator(seq_length, depth, latent_dim)\n",
        "discriminator = build_discriminator(seq_length,depth)\n",
        "gan = compile_gan(generator, discriminator)"
      ],
      "metadata": {
        "id": "Vg8qm4COcKOj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train GAN\n",
        "train_gan(generator, discriminator, gan, path, epochs=2, batch_size=22, latent_dim=latent_dim, demo_noise=demo_noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEbCug03xJk0",
        "outputId": "026cf34e-fa90-468f-882e-43edf2c69796"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAVIDGGTVVNRHSDGNNTGEEVPNGRNNNRNSNIDVGNTFNVTEERDNDDGDDSANRENFRDKDGVDGLRGNVAENLHFKNNSVTRERRTAGRRQNFLLKKLDQTDQNNRSVRNDVLDKVTKRGLVVRSADDLDDDMGD\n",
            "Epoch 1/2; FP 0.0021; FN 0.0020; G_Loss 29.0024\n",
            "MKRLTSVLILLVELKLGLAETAGRVSVEAGSSGGGLLLSRALKKKTISVVLEVNGAGREGVGLNKVLEVEIEVKGLLLLILGEIKDLLSDELEGGGVGEVLQILLAVEIFIGGALGKGGGEAPEKKALKLRGFKLK\n",
            "Epoch 1/2; FP 0.0023; FN 0.0040; G_Loss 15.8881\n",
            "MSLIQKALELLLLVASLLIERKLIGQSKLGSAARSGELGETSQLLLEKQSIKMVAGSAYISALTLSDVDALQDGLLLTPANEEEPEGEALAPTEEREGLPSLSLKSLERRKPPRTAVAEIEKSLLDDLGKISGVNSLLFLALSEL\n",
            "Epoch 1/2; FP 0.0077; FN 0.0117; G_Loss 7.4617\n",
            "MAQKLIRGSLALLLSALPSAESRLSSSASLLPLAGKLSLETFVSILLTLLGLLLSLVAESSSSASESSRELKEELAGGAAATLADLLALGRALPGGSSISSAPGAAEVESASKNLEELKKGLIGSPKVLLLTLEILKLVIVLALG\n",
            "Epoch 1/2; FP 0.0059; FN 0.0093; G_Loss 7.5793\n",
            "MKTLEEDGIVDVEEIAEEVEEQVRKGDAEEIDDKLKEDEVGVAELDEFVEKAVEELIKNVNKVGDDIALLAEDFEIAASEAFVDEEAEFVIDVEKEKEDVEVAAEEANDVKESGEEYVAVDEAKVEVA\n",
            "Epoch 1/2; FP 0.0047; FN 0.0066; G_Loss 7.9170\n",
            "MSFHITILQKQSLGLRLHEIKTELNTKRKEKVPSIEVQEGRDNNFRSGASVAVSIIRTGTGIINDSKRFLKNFKARNRKKRGLHIKVLRTRGF\n",
            "Epoch 1/2; FP 0.0043; FN 0.0054; G_Loss 9.7123\n",
            "MSDMELAIKDLAAAKGKGLAKKGGASAAALHDATTLLAQKLMLNADHGKVKVSGQDAGSHVMAIENKTLAEVDLTSNDVSLIALQLGLLNASE\n",
            "Epoch 1/2; FP 0.0043; FN 0.0051; G_Loss 9.9543\n",
            "ASALNQRVPNVVSSVPLLINSVLSERRRTCVVLNVVLELIDPAVMFKRAVLELKQSRLYLLYRPLLIAELRDVLMVRIR\n",
            "Epoch 1/2; FP 0.0039; FN 0.0041; G_Loss 11.0577\n",
            "MAKQFRKKRVKFIKTKAVKEKVKVKNEEFEQKKEEQFQEDDIEIEFY\n",
            "Epoch 1/2; FP 0.0039; FN 0.0047; G_Loss 11.1427\n",
            "MSGTVLSRRVTCCPGTVLRGRVLVSELLSSAVTSSSITLKRRSISIYLRRIEDASVLRAKKGSRERRADGGGALTGGSLNKSRLFKLGRSGGLQQ\n",
            "Epoch 1/2; FP 0.0038; FN 0.0042; G_Loss 12.7899\n",
            "MSLLILRRLRTELKTRMLKLKQTSAAAELRGAAVLLAASQLAKGRSKQPEIKAGNALQEPELQDAAKAIFVRLMEAELEEEASSALSSLE\n",
            "Epoch 1/2; FP 0.0034; FN 0.0041; G_Loss 12.8956\n",
            "LALFFLAYDLLLLAFRLSTLFALLEELLVFLLFYFNVFV\n",
            "Epoch 1/2; FP 0.0044; FN 0.0047; G_Loss 12.4668\n",
            "FLKWHLRIPAKTA\n",
            "Epoch 2/2; FP 0.0042; FN 0.0063; G_Loss 9.9652\n",
            "VKSTRT\n",
            "Epoch 2/2; FP 0.0018; FN 0.0035; G_Loss 9.2390\n",
            "MPTLSLALRCLLSQLR\n",
            "Epoch 2/2; FP 0.0022; FN 0.0036; G_Loss 10.1780\n",
            "KLASRILKQ\n",
            "Epoch 2/2; FP 0.0013; FN 0.0038; G_Loss 10.0746\n",
            "APVDLLGA\n",
            "Epoch 2/2; FP 0.0020; FN 0.0032; G_Loss 11.3487\n",
            "GISNILR\n",
            "Epoch 2/2; FP 0.0016; FN 0.0042; G_Loss 8.8047\n",
            "LPTDLTFLR\n",
            "Epoch 2/2; FP 0.0018; FN 0.0045; G_Loss 8.9663\n",
            "EKQLV\n",
            "Epoch 2/2; FP 0.0013; FN 0.0038; G_Loss 9.2373\n",
            "RASHRLG\n",
            "Epoch 2/2; FP 0.0013; FN 0.0035; G_Loss 9.6677\n",
            "EDPTLLWG\n",
            "Epoch 2/2; FP 0.0014; FN 0.0046; G_Loss 8.4710\n",
            "QVAAPIAQL\n",
            "Epoch 2/2; FP 0.0013; FN 0.0039; G_Loss 9.0486\n",
            "VPQYVGLFF\n",
            "Epoch 2/2; FP 0.0015; FN 0.0051; G_Loss 7.7837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After GAN is trained\n",
        "def generate_sequences(generator, latent_dim, num_sequences):\n",
        "    noise = (np.random.rand(num_sequences, latent_dim)-0.5)*2\n",
        "    generated_sequences = generator.predict(noise, verbose=0)\n",
        "    return onehot2seq(generated_sequences)\n",
        "\n",
        "def onehot2seq(onehot):\n",
        "    demo_seq = tf.math.argmax(onehot, axis=2)\n",
        "    demo_seq = pep_decoder(demo_seq).numpy().astype('str')\n",
        "    demo_seq = [\"\".join(chars) for chars in demo_seq]\n",
        "    return demo_seq"
      ],
      "metadata": {
        "id": "hlqFCOxXx433"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sequences=10\n",
        "generated_seqs = generate_sequences(generator, latent_dim, num_sequences=num_sequences)\n",
        "generated_seqs"
      ],
      "metadata": {
        "id": "eCogvdXx2I0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89834ed-c03b-4340-f073-bff305827a52"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RAKKAAPLAAVM',\n",
              " 'RLKLPARL',\n",
              " 'DAKKAAPLAIKM',\n",
              " 'RLKLPARL',\n",
              " 'VTDYFGLFW',\n",
              " 'RPIPPPPST',\n",
              " 'VTDYFGLFW',\n",
              " 'RAKKAIPKAAVLMW',\n",
              " 'RAKKAILVAVMQ',\n",
              " 'ALKWAAPYAYKQMLW']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.save(\"PeptideGenerator.keras\")"
      ],
      "metadata": {
        "id": "-A2qBhPPUYPL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.save(\"/content/AMPLify-Feedback/processed_data/GAN_seq/generated_seqs_10\",generated_seqs)"
      ],
      "metadata": {
        "id": "CIjPycwJ3Fbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to One_hot\n",
        "# generated_seqs_one_hot = tf.one_hot(generated_seqs.squeeze(), depth=43)\n",
        "# np.save(\"/content/AMPLify-Feedback/processed_data/GAN_seq/generated_seqs_one_hot_10\",generated_seqs_one_hot)"
      ],
      "metadata": {
        "id": "i-sU4iUtFO51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}